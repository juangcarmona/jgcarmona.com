---
title: "DADS #03: Concurrencia y Paralelismo"
description: ''
pubDate: 2024-09-20
categories: 
  - "proyectos"
tags: 
  - "sw-architecture"
  - "software"
heroImage: "images/image-16.png"
---

## Aplicaciones pr√°cticas, desde el hardware hasta sistemas distribuidos

En este cap√≠tulo de [**DADS**](https://jgcarmona.com/category/dads/), **Dise√±o, Arquitectura y Desarrollo de Software**, te voy a hablar de la **concurrencia y el paralelismo**. De nuevo he recurrido a dos conceptos fundamentales, b√°sicos y a menudo utilizados err√≥neamente, para dise√±ar un viaje trepidante, desde los m√°s rec√≥nditos rincones de una CPU cualquiera hasta su aplicaci√≥n en los sistemas que dan soporte computacional a nuestras aplicaciones y servicios favoritos.

La **concurrencia y el paralelismo** son dos conceptos clave en el desarrollo de software moderno. Entender sus diferencias y aplicaciones es fundamental para dise√±ar sistemas eficientes y escalables. En este art√≠culo, exploraremos juntos, qu√© son, cu√°ndo utilizarlos y c√≥mo se implementan a trav√©s de ejemplos pr√°cticos en hardware, software y sistemas distribuidos.

### **¬øQu√© es la concurrencia?**

La **concurrencia** es la capacidad de un sistema para gestionar **m√∫ltiples tareas** que se alternan en el tiempo, **sin ejecutarse necesariamente al mismo tiempo**. Es √∫til, por ejemplo, para optimizar el uso de recursos en sistemas que manejan operaciones de I/O (entrada/salida) o cuando se requieren muchas tareas activas de manera simult√°nea como por ejemplo en la interfaz de un sistema operativo.

### **¬øQu√© es el paralelismo?**

El **paralelismo** se refiere a la **ejecuci√≥n simult√°nea de varias tareas**, aprovechando m√∫ltiples n√∫cleos o procesadores. Es fundamental cuando se necesita procesar grandes vol√∫menes de datos o ejecutar c√°lculos complejos en **sistemas** con capacidades **multicore** o **distribuidos**.

## **Diferencias en la ejecuci√≥n de tareas**:

Vamos a dejar la tecnolog√≠a durante un momento. Imagina que est√°s en una cocina preparando varios platos, siguiendo varias recetas, al mismo tiempo. Dependiendo de c√≥mo gestiones tus tareas, estar√°s trabajando de forma concurrente, paralela o de ninguna de las dos. Te pongo cuatro ejemplos para que lo entiendas.

1. **Ni Concurrente, Ni Paralelo**:
    - Solo tienes un chef en la cocina, y ese chef **prepara un plato y lo termina antes de comenzar el siguiente**. Primero cocina la sopa y, cuando termina, pasa a hacer la ensalada. Cada receta se realiza de principio a fin sin interrupciones ni solapamientos.

3. **Concurrente, No Paralelo**:
    - Aqu√≠, el chef **alterna entre varias tares de varias recetas.** Comienza a preparar la sopa, luego mientras hierve el caldo, corta los ingredientes de la ensalada. El chef s√≥lo puede realizar una tarea cada vez y como hay procesos que llevan tiempo, como hervir el caldo, aprovecha esos "tiempos muertos" para adelantar trabajo realizando otras tareas de otras recetas **y va progresando en ambos platos a la vez**.

5. **No Concurrente, Paralelo**:
    - Tienes **un equipo de cocina, con** **varios chefs** en la cocina. Uno se encarga exclusivamente de la sopa, y otro de la ensalada. **Ambos trabajan en paralelo**, pero cada uno se enfoca √∫nicamente en su tarea, completando un plato completamente antes de pasar a otro.

7. **Concurrente y Paralelo**:
    - En este caso tambi√©n tienes **varios chefs** en la cocina, y cada uno **alterna entre diferentes platos**. Mientras uno corta los ingredientes de la ensalada, el otro cocina la sopa y tambi√©n ayuda a preparar el postre al mismo tiempo. **Todos los chefs avanzan en varias tareas simult√°neamente**, optimizando el tiempo y los recursos al m√°ximo.

![Diferencias entre la concurrencia y el paralelismo. Infograf√≠a 1. Ejemplo de cocina.](images/1-1024x1024.png)

* * *

# **Aplicaciones de Concurrencia y Paralelismo a Todos los Niveles**

Ahora que entendemos qu√© son la **concurrencia** y el **paralelismo**, podmeos volver a nuestra realidad, la tecnolog√≠a. Vamos a ver c√≥mo se aplican estas dos t√©cnicas en distintos niveles. Exploraremos c√≥mo estos conceptos se implementan en tareas de bajo nivel, como el manejo de recursos de la **CPU** y la memoria, pasando por la **programaci√≥n concurrente** en software, hasta llegar a la **gesti√≥n de cl√∫steres** en **sistemas distribuidos**. Veremos ejemplos pr√°cticos en cada uno de estos niveles, para comprender c√≥mo se ha aplicado la concurrencia y el paralelismo para optimizar diferentes procesos.

* * *

## **Ejemplos a nivel de Hardware**:

### **CPU**

- **Concurrencia**: En una CPU de un solo n√∫cleo, la concurrencia se gestiona mediante t√©cnicas como **time-slicing** (segmentaci√≥n de tiempo). Esto permite que varias tareas compartan el n√∫cleo, alternando su ejecuci√≥n en intervalos cortos de tiempo. Aunque no se ejecutan simult√°neamente, parece que las tareas avanzan al mismo tiempo. Otros mecanismos incluyen **context switching**, que permite a la CPU cambiar entre tareas cuando una est√° en espera.  
      
    **T√©cnicas clave**:
    - **Time-slicing**: La CPU divide el tiempo de procesamiento entre las tareas en curso, alternando r√°pidamente entre ellas.
    
    - **Context switching**: Cambia el estado de una tarea en ejecuci√≥n y almacena su contexto para reanudarla m√°s adelante.
    
    - **Interrupt handling**: La CPU puede detener temporalmente la ejecuci√≥n de una tarea para atender eventos externos prioritarios (por ejemplo, se√±ales de hardware). No son un mecanismo de concurrencia per se, sino m√°s bien un mecanismo de interrupci√≥n para dar paso a una tarea prioritaria.  
          
        

- **Paralelismo**: Con procesadores de m√∫ltiples n√∫cleos, la CPU puede ejecutar varias tareas **simult√°neamente**. Cada n√∫cleo ejecuta una tarea distinta sin tener que alternar entre ellas. Esto es ideal para procesar m√∫ltiples hilos de ejecuci√≥n o tareas independientes al mismo tiempo, aprovechando la capacidad completa del hardware.  
      
    **T√©cnicas clave**:
    - **Multithreading**: Los n√∫cleos pueden manejar varios hilos dentro de una misma tarea o aplicaci√≥n, ejecut√°ndolos en paralelo.
    
    - **Hyper-threading**: Simula la existencia de m√∫ltiples n√∫cleos virtuales dentro de un n√∫cleo f√≠sico, permitiendo que se procesen varias tareas al mismo tiempo.
    
    - **Pipelining**: Divide una tarea en varias etapas que se procesan en paralelo dentro del mismo n√∫cleo, mejorando la eficiencia.
    
    - **Cache coherency**: Las CPU modernas deben mantener coherente la cach√© de cada n√∫cleo, lo que puede afectar la velocidad de ejecuci√≥n paralela. [Merece la pena echarle un vistazo a este art√≠culo](http://frankdenneman.nl/2016/07/11/numa-deep-dive-part-3-cache-coherency/) de Frank Denneman sobre el tema.  
        

* * *

### **Memoria**

- **Concurrencia**: La concurrencia en el manejo de memoria se logra utilizando t√©cnicas como **paging** (paginaci√≥n) y **virtual memory** (memoria virtual), que permiten que diferentes procesos compartan y utilicen la memoria de manera eficiente. La CPU alterna entre las tareas y utiliza la **memoria virtual** para mapear las direcciones de memoria f√≠sica, lo que permite manejar m√°s procesos de los que la memoria f√≠sica permitir√≠a directamente.  
      
    **T√©cnicas clave:**
    - **Memory paging**: T√©cnica que permite que diferentes procesos utilicen bloques de memoria (p√°ginas) que se pueden intercambiar entre la memoria f√≠sica y la memoria virtual para gestionar eficientemente los recursos.
    
    - **Virtual memory**: La memoria virtual permite que un sistema ejecute programas m√°s grandes que la memoria f√≠sica disponible, al usar espacio en disco como extensi√≥n de la RAM.  
          
        

- **Paralelismo**: En arquitecturas de memoria como **NUMA** (Non-Uniform Memory Access), la memoria est√° dividida en diferentes "zonas", cada una de las cuales tiene acceso m√°s r√°pido para un conjunto de n√∫cleos espec√≠ficos. Los procesos en diferentes n√∫cleos pueden acceder a su "memoria local" sin interferencia de otros n√∫cleos, permitiendo un acceso paralelo eficiente. Adem√°s, la **memoria de doble canal** mejora el rendimiento al permitir que dos m√≥dulos de memoria trabajen simult√°neamente.  
      
    **T√©cnicas clave:**
    - **NUMA (Non-Uniform Memory Access)**: Optimiza el acceso paralelo a la memoria asignando zonas espec√≠ficas a cada procesador o n√∫cleo, reduciendo la latencia al acceder a los datos. NUMA **tambi√©n es relevante en sistemas con varios procesadores**, donde cada procesador tiene un bloque de memoria m√°s cercano f√≠sicamente (local), lo que reduce la latencia de acceso. [Aqu√≠ te dejo el enlace al inicio de una serie buen√≠sima, de Frank Denneman, sobre NUMA.](https://frankdenneman.nl/2016/07/06/introduction-2016-numa-deep-dive-series/) Si no lo conoc√≠as, bueno, que sepas que muchos de los avances en Hardware que hacen posible, a su vez, los avances en Inteligencia Artificial, est√°n relacionados con optimizaciones en la utilizaci√≥n de la memoria y NUMA o variantes de NUMA, suelen est√°r presentes.
    
    - **Dual-channel memory**: Permite que dos m√≥dulos de memoria trabajen en paralelo, aumentando el ancho de banda y mejorando la velocidad de acceso a datos.
    
    - **Zoned Namespaces (ZNS)**: En sistemas de almacenamiento avanzados, se divide el espacio en zonas para operaciones de lectura/escritura simult√°neas.

* * *

### Disco

- **Concurrencia**: Mientras el disco recupera informaci√≥n para una tarea (por ejemplo, una operaci√≥n de lectura/escritura), el procesador puede comenzar a trabajar en otra. Se _alterna entre varias tareas_ de entrada/salida (**I/O-bound**), sin que se ejecuten todas al mismo tiempo, _aprovechando los tiempos muertos_ cuando el disco est√° ocupado.  
      
    **T√©cnicas clave:**
    - **I/O Multiplexing**: Permite que varias operaciones de entrada/salida se gestionen de manera eficiente, alternando entre ellas seg√∫n su disponibilidad. Esto es especialmente √∫til en sistemas que manejan muchas conexiones simult√°neas, como bases de datos o servidores web.
    
    - **DMA (Direct Memory Access)**: Esta t√©cnica permite que los dispositivos de hardware, como discos duros o tarjetas de red, accedan directamente a la memoria sin necesidad de que la CPU intervenga en cada operaci√≥n, lo que libera a la CPU para que gestione otras tareas.
    
    - **Write-back caching**: Es una t√©cnica de almacenamiento en cach√© en la que las operaciones de escritura se almacenan temporalmente en la cach√© y se escriben en el disco en un momento posterior. Esto mejora el rendimiento general del sistema al permitir que las operaciones de escritura sean concurrentes.  
          
        

- **Paralelismo**: En sistemas con discos m√∫ltiples (por ejemplo, **RAID** o m√∫ltiples **SSDs**), es posible realizar varias operaciones de _lectura/escritura de forma simult√°nea_, aumentando el rendimiento. Aqu√≠ el paralelismo aprovecha el acceso concurrente a m√∫ltiples dispositivos de almacenamiento para acelerar el procesamiento.  
      
    **T√©cnicas clave:**
    - **RAID**: Agrupaci√≥n de discos que permite el acceso paralelo a los datos, mejorando la velocidad de lectura y escritura a la vez que se asegura la redundancia de los datos.
    
    - **SSDs en paralelo**: Los SSDs permiten paralelizar las operaciones de lectura y escritura a trav√©s de m√∫ltiples canales internos, lo que mejora significativamente la velocidad.
    
    - **NVMe**: La tecnolog√≠a NVMe permite realizar muchas operaciones de I/O en paralelo, aprovechando la arquitectura multicore para ofrecer tiempos de respuesta m√°s r√°pidos. NVMe reduce las colas de espera de I/O al permitir que m√∫ltiples "l√≠neas de comandos" que ejecutan solicitudes de I/O en paralelo, lo que es esencial para aprovechar al m√°ximo la arquitectura multicore moderna. [**He le√≠do aqu√≠**](https://abdulsadeqkhan.com/2022/12/18/what-is-nvme/) que las unidades NVMe generalmente pueden ofrecer una velocidad de lectura y escritura sostenida de 3,5 GB/s en contraste con los SSD SATA que limitan a 600 MB/s.
    
    - **Disk Striping**: T√©cnica que distribuye los datos entre varios discos, permitiendo un acceso simult√°neo a m√∫ltiples bloques de datos.
    
    - **Zoned Namespaces (ZNS)**: Divide el espacio en zonas para realizar operaciones de lectura/escritura simult√°neas, optimizando el rendimiento en discos de gran capacidad. Ya hemos hablado de esta t√©cnica cuando hablamos de la gesit√≥n y el acceso a memoria en paralelo, por eso no te extra√±ar√° si te digo que esta tecnolog√≠a est√° m√°s orientada a **discos SSD de alto rendimiento** y grandes capacidades, como los utilizados en centros de datos por su similitud, a nivel f√≠sico, con las memorias.

* * *

## **Ejemplos a nivel de Software**:

### **Programaci√≥n**

- **Concurrencia**: La programaci√≥n concurrente permite que diferentes partes de un programa se ejecuten de manera intermitente, compartiendo el tiempo de ejecuci√≥n de la CPU. Aunque no se ejecuten simult√°neamente, todas avanzan progresivamente. Esto es especialmente √∫til en aplicaciones que requieren responder a m√∫ltiples eventos (por ejemplo, peticiones HTTP, operaciones I/O).  
      
    **T√©cnicas y tecnolog√≠as clave**:
    - **Async/Await** (C#, JavaScript): La programaci√≥n asincr√≥nica en .NET ha evolucionado significativamente desde la introducci√≥n de `async` y `await` en C# 5, lo que ha simplificado considerablemente su implementaci√≥n. Hoy en d√≠a, frameworks como ASP.NET Core son completamente asincr√≥nicos, y es dif√≠cil evitar el uso de `async` al escribir servicios web. Sin embargo, esto ha generado cierta confusi√≥n sobre las mejores pr√°cticas al usar async, y c√≥mo aplicarlo correctamente. Yo mismo suelo cometer errores y por eso quiero compartir aqu√≠ contigo la mejor gu√≠a que conozco para entender y aplicar correctamente async/await en .Net: **[async/await en ASP.NET Core](https://github.com/davidfowl/AspNetCoreDiagnosticScenarios/blob/master/AsyncGuidance.md)**, de **[David Fowler](https://github.com/davidfowl)**.
    
    - **Event Loop** (JavaScript/Node.js): Se trata de un bucle que gestiona las tareas de forma concurrente, ejecutando eventos uno por uno sin bloquear a otros.
    
    - **Coroutines** (Python, Kotlin): Es un mecanismo eficiente para manejar m√∫ltiples tareas cooperativas que ceden el control de manera expl√≠cita. Son m√°s ligeras que los hilos en t√©rminos de consumo de recursos, dado que no necesitan un contexto completo del sistema operativo. En lenguajes como Kotlin o Python, se usan para manejar operaciones I/O-bound de forma eficiente.
    
    - **Patr√≥n Producer-Consumer:** Es un patr√≥n de dise√±o que facilita la concurrencia al permitir que un grupo de procesos (productores) cree datos o tareas y otro grupo de procesos (consumidores) los procese. Este patr√≥n se utiliza a menudo en sistemas de mensajer√≠a, donde los mensajes son generados por un productor y procesados por un consumidor en paralelo. Se implementa t√≠picamente usando colas en memoria (buffering), y herramientas como **concurrent queues** en Java o **[System.Collections.Concurrent](https://learn.microsoft.com/en-us/dotnet/api/system.collections.concurrent?view=net-8.0)** en C#, que proporciona varias clases de colecciones seguras para hilos.  
          
        

- **Paralelismo**: En programaci√≥n paralela, diferentes partes de un programa pueden ejecutarse simult√°neamente en diferentes n√∫cleos o m√°quinas. Esto es √∫til para tareas que requieren un procesamiento intensivo, como c√°lculos matem√°ticos o simulaciones.  
      
    **T√©cnicas y tecnolog√≠as clave**:
    - **Multithreading**: Crear y gestionar m√∫ltiples hilos en un programa, ejecut√°ndolos en paralelo en diferentes n√∫cleos. por ejemplo:![](images/mYpKomUrixQAAAABJRU5ErkJggg==)
    
    - **Parallel LINQ (PLINQ)** (C#): Un conjunto de funciones que permite la ejecuci√≥n de consultas y operaciones en paralelo de manera sencilla.![](images/cqZI4pg7j8AAAAAABJRU5ErkJggg==)
    
    - **MPI** (Message Passing Interface): Un est√°ndar para realizar computaci√≥n paralela en cl√∫steres de m√°quinas, ideal para procesar tareas en m√∫ltiples nodos.

* * *

### **Sistemas Operativos**

- **Concurrencia**: Un sistema operativo puede alternar entre varias aplicaciones en ejecuci√≥n, compartiendo el tiempo de CPU entre procesos. Esto se logra mediante t√©cnicas como el **scheduling**, que decide qu√© tarea recibe tiempo de CPU en cada momento. Aunque no todas las aplicaciones se ejecutan simult√°neamente, parece que lo hacen debido al cambio r√°pido entre ellas.  
      
    **T√©cnicas y tecnolog√≠as clave**:
    - **Preemptive Multitasking**: El sistema operativo interrumpe la ejecuci√≥n de un proceso para permitir que otro, por lo general m√°s prioritario, se ejecute, maximizando la eficiencia de uso de la CPU.
    
    - **Round Robin Scheduling**: El sistema alterna entre tareas asign√°ndoles un "quantum" o intervalo de tiempo para ejecutar cada tarea.
    
    - **Load Balancing**: Redistribuye din√°micamente las tareas en diferentes recursos (como CPU y memoria) para optimizar el rendimiento.  
          
        

- **Paralelismo**: En un sistema operativo que gestiona m√∫ltiples n√∫cleos, diferentes procesos o hilos pueden ejecutarse en paralelo, aprovechando al m√°ximo la capacidad de procesamiento.  
      
    **T√©cnicas y tecnolog√≠as clave**:
    - **Affinity Scheduling**: Asocia hilos o procesos a n√∫cleos espec√≠ficos para mejorar el rendimiento en sistemas multicore, es decir, este tipo de planificaci√≥n busca mantener los hilos en un mismo n√∫cleo para aprovechar la cach√© usada previamente, minimizando el tiempo de conmutaci√≥n de cach√© entre n√∫cleos.
    
    - **NUMA (Non-Uniform Memory Access)**: Ya hemos hablado de esto m√°s arriba, es un sistema de gesti√≥n de memoria que optimiza la ejecuci√≥n paralela en m√°quinas con m√∫ltiples procesadores, donde cada procesador tiene su propio acceso a la memoria.
    
    - **SMT (Simultaneous Multithreading)**: Permite que un n√∫cleo f√≠sico ejecute varios hilos simult√°neamente, mejorando el rendimiento de procesos paralelos.

* * *

## **Ejemplos a nivel de Sistemas**

### **Servidores Web**

- **Concurrencia**: Los servidores web, como **Kestrel** en **.NET Core** o **Node.js**, permiten manejar m√∫ltiples solicitudes de manera concurrente, alternando entre tareas o utilizando un modelo as√≠ncrono. Los servidores web se aprovechan de las t√©cnicas de programaci√≥n utilizadas en los despliegues.  
      
    **T√©cnicas y tecnolog√≠as clave**:
    - **Async/Await** en **.NET Core** y **Node.js**: Maneja m√∫ltiples solicitudes sin bloquear el flujo principal, ideal para tareas de entrada/salida. (Lo he mencionado antes üòä)
    
    - **Event Loop** en **Node.js**: Administra las conexiones de manera concurrente, manteniendo el servidor receptivo sin necesidad de m√∫ltiples hilos. (Y tambi√©n lo he mencionado antes üòä)  
          
        

- **Paralelismo**: Para manejar muchas conexiones en paralelo, se aprovechan los n√∫cleos del CPU. Esto se puede hacer a trav√©s de procesos y workers que distribuyen las cargas de trabajo. De igual forma, los servidores web se aprovechan del Hardware en el que residen para conseguir el, tan deseado, paralelismo.  
      
    **T√©cnicas y tecnolog√≠as clave**:
    - **Multithreading** en **Kestrel**: Permite que varias solicitudes se ejecuten simult√°neamente en m√∫ltiples n√∫cleos.
    
    - **Node.js Cluster**: Crea m√∫ltiples procesos worker que aprovechan todos los n√∫cleos del procesador para procesar m√∫ltiples solicitudes en paralelo.
    
    - **Servidores Sin Stado (Stateless):** Son esenciales en arquitecturas distribuidas que manejan paralelismo a gran escala. Estos servidores son ideales para **autoescalado** en la nube, ya que cada instancia puede manejar cualquier solicitud sin depender del estado almacenado en memoria local.

* * *

### **Contenedores**

- **Concurrencia**: **Docker** permite ejecutar varios contenedores en un mismo nodo, compartiendo recursos del host y alternando entre tareas dependiendo de la disponibilidad. En este caso, la concurrencia se maneja a nivel de contenedores en el mismo nodo.  
      
    **T√©cnicas y tecnolog√≠as clave**:
    - **Docker**: Varios contenedores pueden ejecutar tareas alternas en el mismo entorno, compartiendo la CPU y la memoria.
    
    - **Namespace Isolation**: Docker a√≠sla procesos en contenedores, permitiendo que varios contenedores se ejecuten de forma independiente, es decir, podr√≠amos tener varias instancias de la misma imagen trabajando en paralelo (Y lo considero concurrencia por que al final, estas instancias consumen los recursos de Hardware del host, aqu√≠ la l√≠nea entre concurrencia y paralelismo es, digamos, difusa)  
          
        

- **Paralelismo**: Con **Kubernetes**, los contenedores se distribuyen entre diferentes nodos de uno o varios cl√∫sters, lo que permite ejecutar varias tareas o servicios de manera simult√°nea. Cada nodo maneja m√∫ltiples pods (instancias) en paralelo, maximizando el rendimiento del cl√∫ster.  
      
    **T√©cnicas y tecnolog√≠as clave**:
    - **Kubernetes Pods**: Los pods permiten ejecutar m√∫ltiples contenedores en diferentes nodos en paralelo.
    
    - **Horizontal Pod Autoscaler**: Escala los pods autom√°ticamente seg√∫n la carga, permitiendo ejecutar m√°s instancias de un servicio en paralelo. En uno de mis √∫ltimos proyectos me hice fan de [KEDA | Kubernetes Event-driven Autoscaling](https://keda.sh/), con el que se pod√≠a incrementar el numero de instancias de pods, workers, de acuerdo a muchos par√°mteros, por ejemplo, el n√∫mero de mensajes en una cola de mensajer√≠a. No os pod√©is imaginar la mejora global de rendimiento y el ahorro de coste que conseguimos con esto. ¬°Las dos cosas!
    
    - **ReplicaSets**: Garantizan que un n√∫mero deseado de pods se ejecute simult√°neamente en un cl√∫ster. Tambi√©n tambi√©n permiten el **self-healing** de los pods en caso de fallo, reponiendo autom√°ticamente cualquier pod que se pierda, por el motivo que sea.

* * *

### **Cloud Computing y Sistemas Distribuidos**

- **Concurrencia**: En la nube, plataformas como **Azure App Services** permiten manejar m√∫ltiples solicitudes concurrentes mediante la asignaci√≥n din√°mica de recursos. Las aplicaciones pueden alternar entre tareas concurrentes seg√∫n la carga, sin necesidad de intervenci√≥n manual.  
      
    **T√©cnicas y tecnolog√≠as clave**:
    - **Azure App Services**: Manejan m√∫ltiples peticiones de usuarios concurrentemente, utilizando balanceadores de carga que distribuyen las solicitudes.
    
    - **Task Scheduling** en **Azure Functions**: Permite que las funciones en la nube se ejecuten de forma concurrente y bajo demanda, respondiendo a eventos o solicitudes.  
          
        

- **Paralelismo**: En sistemas distribuidos y cl√∫steres en la nube, el paralelismo se logra ejecutando servicios o instancias en m√∫ltiples nodos. Esto permite distribuir el trabajo entre diferentes zonas geogr√°ficas o grupos de m√°quinas, aumentando la capacidad y la disponibilidad.  
      
    **T√©cnicas y tecnolog√≠as clave**:
    - **Azure VM Scale Sets**: Proporcionan escalabilidad paralela para manejar grandes vol√∫menes de tr√°fico, ejecutando m√∫ltiples instancias de m√°quinas virtuales simult√°neamente.
    
    - **Azure Batch**: Permite la ejecuci√≥n en paralelo de tareas a gran escala, dividiendo grandes procesos en peque√±os trabajos que se distribuyen en m√∫ltiples nodos.

* * *

## **De Principio a Fin: Lo que Concurrencia y Paralelismo nos Ense√±an**

Espero que este recorrido por las distintas capas de la tecnolog√≠a te haya resultado tan emocionante como a m√≠. Desde los detalles del hardware hasta la gesti√≥n de sistemas distribuidos, hemos visto c√≥mo la concurrencia y el paralelismo son claves para optimizar el rendimiento y dise√±ar soluciones escalables. ¬øEras consciente de que la concurrencia y el paralelismo estaban presentes en todos estos niveles? Para ayudarte a visualizar todo lo que hemos cubierto, aqu√≠ tienes una infograf√≠a que resume las t√©cnicas y tecnolog√≠as clave de **concurrencia y paralelismo** en cada capa del que afectan al dise√±o, la arqutiectura y el desarrollo de software.

![Aplicaciones de concurrencia y paralelismo. Infograf√≠a 2. Ejemplo de cocina.](images/2-1024x1024.png)

Ahora que conoces estos conceptos a fondo, ¬øest√°s listo para aplicarlos y hacer que tus proyectos brillen con todo su potencial? ¬°Es hora de poner manos a la obra!

## **Si quieres llevar tus proyectos o tu carrera al siguiente nivel...**

Si este an√°lisis te ha inspirado y te gustar√≠a aplicar estas t√©cnicas en tus proyectos, mejorar la arquitectura de tus sistemas o desarrollar tus habilidades como l√≠der t√©cnico, **[estoy aqu√≠ para ayudarte](https://jgcarmona.com/contact/)**. Ya sea a trav√©s de **[coaching personalizado](https://jgcarmona.com/coaching/)** para potenciar tu carrera como arquitecto o l√≠der t√©cnico, o mediante **[consultor√≠a especializada](https://jgcarmona.com/consultoria-especializada-en-desarrollo-de-software/)** para optimizar y escalar tus soluciones tecnol√≥gicas, juntos podemos transformar tus ideas en realidades de alto impacto.

**[Contacta conmigo](https://jgcarmona.com/contact/) y [trabajemos juntos para llevar](https://jgcarmona.com/coaching/)** [tu carrera](https://jgcarmona.com/coaching/) y [**tus proyectos**](https://jgcarmona.com/consultoria-especializada-en-desarrollo-de-software/) al siguiente nivel.

## **Pr√≥ximamente tambi√©n en YouTube**

Este tema ser√° parte del pr√≥ximo cap√≠tulo de mi serie [**DADS** (Dise√±o, Arquitectura y Desarrollo de Software) en YouTub](https://www.youtube.com/playlist?list=PLquujPA7EWzNBPpN8vcN4XkvuGxIxYyu2)[e](https://www.youtube.com/playlist?list=PLquujPA7EWzNBPpN8vcN4XkvuGxIxYyu2), donde profundizaremos a√∫n m√°s sobre la concurrencia y el paralelismo. Si est√°s interesado en ver ejemplos pr√°cticos y explicaciones detalladas, suscr√≠bete a **[mi canal](https://www.youtube.com/@juangcarmona)** para mantenerte al tanto. Muy pronto estar√° disponible.

[![](images/Coming-Soon-1.png)](https://www.youtube.com/@juangcarmona)

No te pierdas el estreno.
